{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Method 1: Convolutional Neural Network\n",
    "\n",
    "For the first method, we decided to use a CNN which benefits from images as input. We can apply filters and kernel functions to reduce the dimensionality of our images."
   ],
   "id": "644a4f4217f72d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Getting Started\n",
    "\n",
    "First, we import the libraries we are going to use to train our Convolutional Neural Network. We will also define the environments relative paths and some utility/auxiliary functions."
   ],
   "id": "5e5d36036a0962b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.1 Imports",
   "id": "45b6022b4f99ede3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T20:07:47.186934Z",
     "start_time": "2025-06-08T20:07:45.044890Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ],
   "id": "89a820df12e2c868",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.2 Environment Configuration",
   "id": "74dcf3b354208d37"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T20:07:48.393049Z",
     "start_time": "2025-06-08T20:07:48.387587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setting the path of the training dataset (that was already provided to you)\n",
    "\n",
    "running_local = True if os.getenv('JUPYTERHUB_USER') is None else False\n",
    "DATASET_PATH = \"../data/sign_lang_train\"\n",
    "\n",
    "# Set the location of the dataset\n",
    "if running_local:\n",
    "    # If running on your local machine, the sign_lang_train folder's path should be specified here\n",
    "    local_path = \"../data/sign_lang_train\"\n",
    "    if os.path.exists(local_path):\n",
    "        DATASET_PATH = local_path\n",
    "else:\n",
    "    # If running on the Jupyter hub, this data folder is already available\n",
    "    # You DO NOT need to upload the data!\n",
    "    DATASET_PATH = \"/data/mlproject22/sign_lang_train\"\n",
    "\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), '..', 'src'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)"
   ],
   "id": "abf06a601549e665",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.3 Auxiliary Functions",
   "id": "7bbb02ef5c68dedc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T20:07:49.640015Z",
     "start_time": "2025-06-08T20:07:49.634677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Auxiliary function\n",
    "def read_csv(csv_file):\n",
    "    with open(csv_file, newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        data = list(reader)\n",
    "    return data"
   ],
   "id": "82a359be95167c0a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.4 Load the Dataset (Sign-Languages)",
   "id": "1e867dfa916be7c1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T20:07:50.606795Z",
     "start_time": "2025-06-08T20:07:50.599976Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from string import ascii_lowercase\n",
    "\n",
    "class SignLangDataset(Dataset):\n",
    "    \"\"\"Sign language dataset\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, root_dir, class_index_map=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data = read_csv(os.path.join(root_dir,csv_file))\n",
    "        self.root_dir = root_dir\n",
    "        self.class_index_map = class_index_map\n",
    "        self.transform = transform\n",
    "        # List of class names in order\n",
    "        self.class_names = list(map(str, list(range(10)))) + list(ascii_lowercase)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Calculates the length of the dataset-\n",
    "        \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns one sample (dict consisting of an image and its label)\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Read the image and labels\n",
    "        image_path = os.path.join(self.root_dir, self.data[idx][1])\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        # The Shape of the image should be H, W, C where C=1\n",
    "        image = np.expand_dims(image, 0)\n",
    "        # The label is the index of the class name in the list ['0','1',...,'9','a','b',...'z']\n",
    "        # because we should have integer labels in the range 0-35 (for 36 classes)\n",
    "        label = self.class_names.index(self.data[idx][0])\n",
    "\n",
    "        sample = {'image': image, 'label': label}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ],
   "id": "37e8746949d709cc",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.5 Import the Network",
   "id": "2323d7454bc790f6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T20:07:51.607495Z",
     "start_time": "2025-06-08T20:07:51.560854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.conv_nn import ConvNN\n",
    "\n",
    "#ConvNN?\n",
    "#ConvNN??\n",
    "#help(ConvNN)\n",
    "\n",
    "cnn = ConvNN().float()\n",
    "cnn.train()\n",
    "\n",
    "filename = \"../models/cnn_weights.pt\"\n",
    "\n",
    "if not os.path.exists(filename):\n",
    "    cnn.init_weights()\n",
    "    torch.save(cnn.state_dict(), filename)\n",
    "    print(f\"Initialized weights saved to '{filename}'\")\n",
    "else:\n",
    "    print(f\"File '{filename}' already exists, skipping save.\")\n",
    "\n",
    "cnn.load_state_dict(torch.load(\"../models/cnn_weights.pt\"))\n",
    "\n",
    "print(\"ConvNN imported successfully!\")"
   ],
   "id": "ead7e0139e378ead",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File '../models/cnn_weights.pt' already exists, skipping save.\n",
      "ConvNN imported successfully!\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Train the model",
   "id": "4e55a0707c671f91"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.1 Use dataloader to create batches for dataset\n",
    "\n",
    "First, we will load our dataset by using a dataloader and test if everything works well by printing a random batch."
   ],
   "id": "e564bbce59d08358"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T20:07:52.934445Z",
     "start_time": "2025-06-08T20:07:52.922780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = SignLangDataset(csv_file='labels.csv', root_dir='../data/sign_lang_train/')\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ],
   "id": "9ba2264ca94010f5",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T20:07:53.595321Z",
     "start_time": "2025-06-08T20:07:53.577802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for batch in dataloader:\n",
    "    images = batch['image']\n",
    "    labels = batch['label']\n",
    "    for image, label in zip(images, labels):\n",
    "        print(f\"Image-Dim: {image.shape} is {label}\")\n",
    "    break\n",
    "print(\"Done!\")"
   ],
   "id": "69a1e87be0cc9479",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image-Dim: torch.Size([1, 128, 128]) is 30\n",
      "Image-Dim: torch.Size([1, 128, 128]) is 12\n",
      "Image-Dim: torch.Size([1, 128, 128]) is 6\n",
      "Image-Dim: torch.Size([1, 128, 128]) is 6\n",
      "Image-Dim: torch.Size([1, 128, 128]) is 11\n",
      "Image-Dim: torch.Size([1, 128, 128]) is 29\n",
      "Image-Dim: torch.Size([1, 128, 128]) is 20\n",
      "Image-Dim: torch.Size([1, 128, 128]) is 6\n",
      "Image-Dim: torch.Size([1, 128, 128]) is 33\n",
      "Image-Dim: torch.Size([1, 128, 128]) is 12\n",
      "Image-Dim: torch.Size([1, 128, 128]) is 32\n",
      "Image-Dim: torch.Size([1, 128, 128]) is 16\n",
      "Image-Dim: torch.Size([1, 128, 128]) is 4\n",
      "Image-Dim: torch.Size([1, 128, 128]) is 23\n",
      "Image-Dim: torch.Size([1, 128, 128]) is 25\n",
      "Image-Dim: torch.Size([1, 128, 128]) is 15\n",
      "Image-Dim: torch.Size([1, 128, 128]) is 21\n",
      "Image-Dim: torch.Size([1, 128, 128]) is 30\n",
      "Image-Dim: torch.Size([1, 128, 128]) is 4\n",
      "Image-Dim: torch.Size([1, 128, 128]) is 35\n",
      "Image-Dim: torch.Size([1, 128, 128]) is 6\n",
      "Image-Dim: torch.Size([1, 128, 128]) is 32\n",
      "Image-Dim: torch.Size([1, 128, 128]) is 21\n",
      "Image-Dim: torch.Size([1, 128, 128]) is 6\n",
      "Image-Dim: torch.Size([1, 128, 128]) is 21\n",
      "Image-Dim: torch.Size([1, 128, 128]) is 6\n",
      "Image-Dim: torch.Size([1, 128, 128]) is 25\n",
      "Image-Dim: torch.Size([1, 128, 128]) is 12\n",
      "Image-Dim: torch.Size([1, 128, 128]) is 15\n",
      "Image-Dim: torch.Size([1, 128, 128]) is 10\n",
      "Image-Dim: torch.Size([1, 128, 128]) is 11\n",
      "Image-Dim: torch.Size([1, 128, 128]) is 22\n",
      "Done!\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.2 Hyperparameters",
   "id": "c43241a2568e7000"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T20:18:15.775636Z",
     "start_time": "2025-06-08T20:18:15.773587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "EPOCHS = 5\n",
    "BATCH_SIZE = 32"
   ],
   "id": "7b3d6565eebeeefc",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T20:18:16.385291Z",
     "start_time": "2025-06-08T20:18:16.382546Z"
    }
   },
   "cell_type": "code",
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=0.001)"
   ],
   "id": "7c6878f5c7892d3e",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-08T20:24:32.190070Z",
     "start_time": "2025-06-08T20:23:44.177257Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trn_corr = 0\n",
    "trn_total = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        images = batch['image'].float()\n",
    "        labels = batch['label']\n",
    "        batch_size = images.size(0)\n",
    "\n",
    "        y_pred = cnn(images)\n",
    "        loss = criterion(y_pred, labels)\n",
    "\n",
    "        predicted = torch.max(y_pred, 1)[1]\n",
    "        batch_corr = (predicted == labels).sum()\n",
    "        trn_corr += batch_corr\n",
    "        trn_total += batch_size\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            batch_acc = batch_corr / batch_size * 100\n",
    "            overall_acc = trn_corr / trn_total * 100\n",
    "            print(f\"Epoch {epoch},\\tBatch {i},\\tloss: {loss.item()}:.8f,\\tBatch Accuracy: {batch_acc:.4f}%,\\tOverall Accuracy: {overall_acc:.4f}%\")"
   ],
   "id": "5a9626760512f069",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0,\tBatch 0,\tloss: 0.22706615924835205:.8f,\tBatch Accuracy: 90.6250%,\tOverall Accuracy: 90.6250%\n",
      "Epoch 0,\tBatch 50,\tloss: 0.026161380112171173:.8f,\tBatch Accuracy: 100.0000%,\tOverall Accuracy: 95.9559%\n",
      "Epoch 0,\tBatch 100,\tloss: 0.17253613471984863:.8f,\tBatch Accuracy: 93.7500%,\tOverall Accuracy: 95.0186%\n",
      "Epoch 0,\tBatch 150,\tloss: 0.3576127886772156:.8f,\tBatch Accuracy: 93.7500%,\tOverall Accuracy: 95.0331%\n",
      "Epoch 0,\tBatch 200,\tloss: 0.09526198357343674:.8f,\tBatch Accuracy: 96.8750%,\tOverall Accuracy: 95.1026%\n",
      "Epoch 0,\tBatch 250,\tloss: 0.16261379420757294:.8f,\tBatch Accuracy: 93.7500%,\tOverall Accuracy: 95.3561%\n",
      "Epoch 0,\tBatch 300,\tloss: 0.16095830500125885:.8f,\tBatch Accuracy: 93.7500%,\tOverall Accuracy: 95.2450%\n",
      "Epoch 1,\tBatch 0,\tloss: 0.0854441374540329:.8f,\tBatch Accuracy: 96.8750%,\tOverall Accuracy: 95.2430%\n",
      "Epoch 1,\tBatch 50,\tloss: 0.20213688910007477:.8f,\tBatch Accuracy: 96.8750%,\tOverall Accuracy: 95.3059%\n",
      "Epoch 1,\tBatch 100,\tloss: 0.08905906975269318:.8f,\tBatch Accuracy: 96.8750%,\tOverall Accuracy: 95.3454%\n",
      "Epoch 1,\tBatch 150,\tloss: 0.2389085292816162:.8f,\tBatch Accuracy: 93.7500%,\tOverall Accuracy: 95.4796%\n",
      "Epoch 1,\tBatch 200,\tloss: 0.0768275186419487:.8f,\tBatch Accuracy: 93.7500%,\tOverall Accuracy: 95.4754%\n",
      "Epoch 1,\tBatch 250,\tloss: 0.18713125586509705:.8f,\tBatch Accuracy: 90.6250%,\tOverall Accuracy: 95.5285%\n",
      "Epoch 1,\tBatch 300,\tloss: 0.16657133400440216:.8f,\tBatch Accuracy: 90.6250%,\tOverall Accuracy: 95.6763%\n",
      "Epoch 2,\tBatch 0,\tloss: 0.04856303334236145:.8f,\tBatch Accuracy: 100.0000%,\tOverall Accuracy: 95.6838%\n",
      "Epoch 2,\tBatch 50,\tloss: 0.12988930940628052:.8f,\tBatch Accuracy: 93.7500%,\tOverall Accuracy: 95.7412%\n",
      "Epoch 2,\tBatch 100,\tloss: 0.0634462758898735:.8f,\tBatch Accuracy: 96.8750%,\tOverall Accuracy: 95.6755%\n",
      "Epoch 2,\tBatch 150,\tloss: 0.16174229979515076:.8f,\tBatch Accuracy: 93.7500%,\tOverall Accuracy: 95.7217%\n",
      "Epoch 2,\tBatch 200,\tloss: 0.04324331507086754:.8f,\tBatch Accuracy: 100.0000%,\tOverall Accuracy: 95.7351%\n",
      "Epoch 2,\tBatch 250,\tloss: 0.03481142967939377:.8f,\tBatch Accuracy: 100.0000%,\tOverall Accuracy: 95.7725%\n",
      "Epoch 2,\tBatch 300,\tloss: 0.07637281715869904:.8f,\tBatch Accuracy: 96.8750%,\tOverall Accuracy: 95.7678%\n",
      "Epoch 3,\tBatch 0,\tloss: 0.12008662521839142:.8f,\tBatch Accuracy: 93.7500%,\tOverall Accuracy: 95.7691%\n",
      "Epoch 3,\tBatch 50,\tloss: 0.1145145371556282:.8f,\tBatch Accuracy: 96.8750%,\tOverall Accuracy: 95.8073%\n",
      "Epoch 3,\tBatch 100,\tloss: 0.10258254408836365:.8f,\tBatch Accuracy: 93.7500%,\tOverall Accuracy: 95.8695%\n",
      "Epoch 3,\tBatch 150,\tloss: 0.175650954246521:.8f,\tBatch Accuracy: 96.8750%,\tOverall Accuracy: 95.9170%\n",
      "Epoch 3,\tBatch 200,\tloss: 0.026974491775035858:.8f,\tBatch Accuracy: 100.0000%,\tOverall Accuracy: 95.9320%\n",
      "Epoch 3,\tBatch 250,\tloss: 0.042882561683654785:.8f,\tBatch Accuracy: 96.8750%,\tOverall Accuracy: 95.9700%\n",
      "Epoch 3,\tBatch 300,\tloss: 0.2874540388584137:.8f,\tBatch Accuracy: 90.6250%,\tOverall Accuracy: 95.9583%\n",
      "Epoch 4,\tBatch 0,\tloss: 0.034187376499176025:.8f,\tBatch Accuracy: 100.0000%,\tOverall Accuracy: 95.9615%\n",
      "Epoch 4,\tBatch 50,\tloss: 0.2523287832736969:.8f,\tBatch Accuracy: 93.7500%,\tOverall Accuracy: 95.9804%\n",
      "Epoch 4,\tBatch 100,\tloss: 0.22551226615905762:.8f,\tBatch Accuracy: 93.7500%,\tOverall Accuracy: 96.0240%\n",
      "Epoch 4,\tBatch 150,\tloss: 0.07210156321525574:.8f,\tBatch Accuracy: 96.8750%,\tOverall Accuracy: 96.0668%\n",
      "Epoch 4,\tBatch 200,\tloss: 0.019729172810912132:.8f,\tBatch Accuracy: 100.0000%,\tOverall Accuracy: 96.1419%\n",
      "Epoch 4,\tBatch 250,\tloss: 0.1414014846086502:.8f,\tBatch Accuracy: 96.8750%,\tOverall Accuracy: 96.1563%\n",
      "Epoch 4,\tBatch 300,\tloss: 0.3930230140686035:.8f,\tBatch Accuracy: 90.6250%,\tOverall Accuracy: 96.1760%\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d8cb68914aedfa23"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
